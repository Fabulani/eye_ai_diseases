{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.eye_dataset import *\n",
    "from eye_classifier import *\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Dataset\n",
    "\n",
    "We use the utility class EyeImageDataset() to load the training dataset based on the metadata CSV file and\n",
    "the target image folder\n",
    "\n",
    "The training images will be loaded as needed due to memory constraints.\n",
    "\n",
    "First, we resize our images to 512x512, so we apply some transforms on our dataset.\n",
    "\n",
    "Next, we normalize the dataset and then it is ready to be trained / tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../../data\"\n",
    "image_dir_training = f\"{base_dir}/ODIR-5K/training\"\n",
    "image_dir_testing = f\"{base_dir}/ODIR-5K/testing\"\n",
    "csv_file = f'{base_dir}/ODIR-5K/data.csv'\n",
    "\n",
    "input_size = 512\n",
    "\n",
    "apply_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=input_size),\n",
    "    transforms.CenterCrop(size=input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds = EyeImageDataset(root=image_dir_training, data_info_csv_file=csv_file, transform=apply_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "To build the model, we use the utility superclass EyeClassifier. \n",
    "\n",
    "We create a subclass named ResnetEyeClassifier and then we feed it with the intended model on its __init__ constructor.\n",
    "\n",
    "Here we're building some CNN as the first layers\n",
    "\n",
    "The last layer will not have a transfer function because we opted to use CrossEntropyLoss as the loss function. \n",
    "\n",
    "You can change the loss function by using set_loss_function() if you wish to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomEyeClassifier(\n",
      "  (layer 1): Conv2d(3, 6, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (layer 2): MaxPool2d(kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
      "  (layer 3): Dropout(p=0.5, inplace=False)\n",
      "  (layer 4): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (layer 5): MaxPool2d(kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
      "  (layer 6): Dropout(p=0.5, inplace=False)\n",
      "  (layer 7): Linear(in_features=13456, out_features=84, bias=True)\n",
      "  (layer 8): Linear(in_features=84, out_features=42, bias=True)\n",
      "  (layer 9): Linear(in_features=42, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CustomEyeClassifier(EyeClassifier):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super(CustomEyeClassifier, self).__init__(model=[\n",
    "\n",
    "            (nn.Conv2d(in_channels=3, out_channels=6,\n",
    "                       kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), dilation=(1, 1)),\n",
    "             TransferFunction.NotApplicable),\n",
    "\n",
    "            (nn.MaxPool2d(\n",
    "                kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), dilation=(1, 1)),\n",
    "             TransferFunction.NotApplicable),\n",
    "\n",
    "            (nn.Dropout(),\n",
    "             TransferFunction.NotApplicable),\n",
    "\n",
    "            (nn.Conv2d(in_channels=6, out_channels=16,\n",
    "                       kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), dilation=(1, 1)),\n",
    "             TransferFunction.NotApplicable),\n",
    "\n",
    "            (nn.MaxPool2d(\n",
    "                kernel_size=(5, 5), stride=(2, 2), padding=(0, 0), dilation=(1, 1)),\n",
    "             TransferFunction.NotApplicable),\n",
    "\n",
    "            (nn.Dropout(),\n",
    "             TransferFunction.NotApplicable),\n",
    "\n",
    "            (nn.Linear(in_features=13456, out_features=84),\n",
    "             TransferFunction.Relu),\n",
    "\n",
    "            (nn.Linear(in_features=84, out_features=42),\n",
    "             TransferFunction.Relu),\n",
    "\n",
    "            (nn.Linear(in_features=42, out_features=num_classes),\n",
    "             TransferFunction.NotApplicable),\n",
    "        ])\n",
    "\n",
    "nn = CustomEyeClassifier(num_classes=len(ds.classes))\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Training the model is very easy. All we have to do is to call train_model() passing the EyeImageDataset object. \n",
    "\n",
    "If you're having I/O constraints, you can use the set_buffer_size(n) method to cache n images in memory. Be aware that you cant use it\n",
    "along with shuffle, because the cache will be constantly invalidated, unless you have a lot of memory and cache all of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training (1%) epoch 1/100, loss = 2.6364\n",
      "training (2%) epoch 2/100, loss = 2.1246\n",
      "training (3%) epoch 3/100, loss = 0.7268\n",
      "training (4%) epoch 4/100, loss = 1.6911\n",
      "training (5%) epoch 5/100, loss = 1.3497\n",
      "training (6%) epoch 6/100, loss = 0.8989\n",
      "training (7%) epoch 7/100, loss = 1.1256\n",
      "training (8%) epoch 8/100, loss = 2.0137\n",
      "training (9%) epoch 9/100, loss = 2.2820\n",
      "training (10%) epoch 10/100, loss = 1.3867\n",
      "training (11%) epoch 11/100, loss = 0.6704\n",
      "training (12%) epoch 12/100, loss = 0.7619\n"
     ]
    }
   ],
   "source": [
    "nn.train_model(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "Testing the model is similar to training it. All we have to do is to call test_model() passing a EyeImageDataset object pointing to the test dataset images. \n",
    "\n",
    "If you're having I/O constraints, you can use the set_buffer_size(n) method to cache n images in memory. Be aware that you cant use it\n",
    "along with shuffle, because the cache will be constantly invalidated, unless you have a lot of memory and cache all of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 512\n",
    "\n",
    "apply_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=input_size),\n",
    "    transforms.CenterCrop(size=input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds = EyeImageDataset(root=image_dir_testing, data_info_csv_file=csv_file, transform=apply_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_buffer_size(16)\n",
    "nn.test_model(ds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
