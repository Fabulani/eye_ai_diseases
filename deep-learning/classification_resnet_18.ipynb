{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.eye_dataset import *\n",
    "from eye_classifier import *\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading input dataset\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../../data\"\n",
    "\n",
    "image_dir_training = f\"{base_dir}/ODIR-5K/training\"\n",
    "#image_dir_training = f\"{base_dir}/preprocessed_images\"\n",
    "image_dir_testing = f\"{base_dir}/ODIR-5K/testing\"\n",
    "csv_file = f'{base_dir}/ODIR-5K/data.csv'\n",
    "\n",
    "print ('reading input dataset')\n",
    "input_size = 224\n",
    "\n",
    "apply_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=input_size),\n",
    "    transforms.CenterCrop(size=input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# apply_transforms = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "ds = EyeImageDataset(root=image_dir_training, data_info_csv_file=csv_file, transform=apply_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResnetEyeClassifier(\n",
      "  (layer 1): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (layer 2): Linear(in_features=1000, out_features=84, bias=True)\n",
      "  (layer 3): Linear(in_features=84, out_features=42, bias=True)\n",
      "  (layer 4): Linear(in_features=42, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ResnetEyeClassifier(EyeClassifier):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super(ResnetEyeClassifier, self).__init__(model=[\n",
    "\n",
    "            (models.resnet18(pretrained=True), TransferFunction.NotApplicable),\n",
    "\n",
    "            (nn.Linear(in_features=1000, out_features=84),\n",
    "             TransferFunction.Relu),\n",
    "\n",
    "            (nn.Linear(in_features=84, out_features=42),\n",
    "             TransferFunction.Relu),\n",
    "\n",
    "            (nn.Linear(in_features=42, out_features=num_classes),\n",
    "             TransferFunction.NotApplicable),\n",
    "        ])\n",
    "\n",
    "\n",
    "nn = ResnetEyeClassifier(num_classes=len(ds.classes))\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training (1%) epoch 1/100, loss = 2.0092\n",
      "training (2%) epoch 2/100, loss = 1.2397\n",
      "training (3%) epoch 3/100, loss = 0.8970\n",
      "training (4%) epoch 4/100, loss = 1.9033\n",
      "training (5%) epoch 5/100, loss = 0.6805\n",
      "training (6%) epoch 6/100, loss = 0.7784\n",
      "training (7%) epoch 7/100, loss = 0.7446\n",
      "training (8%) epoch 8/100, loss = 0.8137\n",
      "training (9%) epoch 9/100, loss = 1.1901\n",
      "training (10%) epoch 10/100, loss = 0.7632\n",
      "training (11%) epoch 11/100, loss = 0.4135\n",
      "training (12%) epoch 12/100, loss = 0.5246\n",
      "training (13%) epoch 13/100, loss = 0.7102\n",
      "training (14%) epoch 14/100, loss = 0.2448\n",
      "training (15%) epoch 15/100, loss = 0.8478\n",
      "training (16%) epoch 16/100, loss = 1.0685\n",
      "training (17%) epoch 17/100, loss = 0.0406\n",
      "training (18%) epoch 18/100, loss = 0.3717\n",
      "training (19%) epoch 19/100, loss = 0.0949\n",
      "training (20%) epoch 20/100, loss = 0.0216\n",
      "training (21%) epoch 21/100, loss = 0.4898\n",
      "training (22%) epoch 22/100, loss = 0.0799\n",
      "training (23%) epoch 23/100, loss = 0.0597\n",
      "training (24%) epoch 24/100, loss = 0.0285\n",
      "training (25%) epoch 25/100, loss = 0.5236\n",
      "training (26%) epoch 26/100, loss = 0.0065\n",
      "training (27%) epoch 27/100, loss = 0.2577\n",
      "training (28%) epoch 28/100, loss = 0.0112\n",
      "training (29%) epoch 29/100, loss = 0.3493\n",
      "training (30%) epoch 30/100, loss = 0.0203\n",
      "training (31%) epoch 31/100, loss = 0.7050\n",
      "training (32%) epoch 32/100, loss = 0.0402\n",
      "training (33%) epoch 33/100, loss = 0.2286\n",
      "training (34%) epoch 34/100, loss = 0.9859\n",
      "training (35%) epoch 35/100, loss = 0.0104\n",
      "training (36%) epoch 36/100, loss = 0.3471\n",
      "training (37%) epoch 37/100, loss = 0.0195\n",
      "training (38%) epoch 38/100, loss = 0.0002\n",
      "training (39%) epoch 39/100, loss = 0.0160\n",
      "training (40%) epoch 40/100, loss = 0.0029\n",
      "training (41%) epoch 41/100, loss = 0.4908\n",
      "training (42%) epoch 42/100, loss = 0.0489\n",
      "training (43%) epoch 43/100, loss = 0.1086\n",
      "training (44%) epoch 44/100, loss = 0.0006\n",
      "training (45%) epoch 45/100, loss = 0.6685\n",
      "training (46%) epoch 46/100, loss = 0.0395\n",
      "training (47%) epoch 47/100, loss = 0.4036\n",
      "training (48%) epoch 48/100, loss = 0.0215\n",
      "training (49%) epoch 49/100, loss = 0.0041\n",
      "training (50%) epoch 50/100, loss = 0.0044\n",
      "training (51%) epoch 51/100, loss = 0.0006\n",
      "training (52%) epoch 52/100, loss = 0.0478\n",
      "training (53%) epoch 53/100, loss = 0.0004\n",
      "training (54%) epoch 54/100, loss = 0.0061\n",
      "training (55%) epoch 55/100, loss = 0.4614\n",
      "training (56%) epoch 56/100, loss = 0.0016\n",
      "training (57%) epoch 57/100, loss = 0.0002\n",
      "training (58%) epoch 58/100, loss = 0.0002\n",
      "training (59%) epoch 59/100, loss = 0.3468\n",
      "training (60%) epoch 60/100, loss = 0.3655\n",
      "training (61%) epoch 61/100, loss = 0.0112\n",
      "training (62%) epoch 62/100, loss = 0.0006\n",
      "training (63%) epoch 63/100, loss = 0.0009\n",
      "training (64%) epoch 64/100, loss = 0.0005\n",
      "training (65%) epoch 65/100, loss = 0.4163\n",
      "training (66%) epoch 66/100, loss = 0.0001\n",
      "training (67%) epoch 67/100, loss = 0.0015\n",
      "training (68%) epoch 68/100, loss = 0.0075\n",
      "training (69%) epoch 69/100, loss = 0.0002\n",
      "training (70%) epoch 70/100, loss = 0.5113\n",
      "training (71%) epoch 71/100, loss = 0.3496\n",
      "training (72%) epoch 72/100, loss = 0.3480\n",
      "training (73%) epoch 73/100, loss = 0.0008\n",
      "training (74%) epoch 74/100, loss = 0.0016\n",
      "training (75%) epoch 75/100, loss = 0.0015\n",
      "training (76%) epoch 76/100, loss = 0.3500\n",
      "training (77%) epoch 77/100, loss = 0.0003\n",
      "training (78%) epoch 78/100, loss = 0.0020\n",
      "training (79%) epoch 79/100, loss = 0.0217\n",
      "training (80%) epoch 80/100, loss = 0.3546\n",
      "training (81%) epoch 81/100, loss = 0.0016\n",
      "training (82%) epoch 82/100, loss = 0.0110\n",
      "training (83%) epoch 83/100, loss = 0.0466\n",
      "training (84%) epoch 84/100, loss = 0.0000\n",
      "training (85%) epoch 85/100, loss = 0.0003\n",
      "training (86%) epoch 86/100, loss = 0.3478\n",
      "training (87%) epoch 87/100, loss = 0.3557\n",
      "training (88%) epoch 88/100, loss = 0.0108\n",
      "training (89%) epoch 89/100, loss = 0.0031\n",
      "training (90%) epoch 90/100, loss = 0.2404\n",
      "training (91%) epoch 91/100, loss = 0.3470\n",
      "training (92%) epoch 92/100, loss = 0.0014\n",
      "training (93%) epoch 93/100, loss = 0.0000\n",
      "training (94%) epoch 94/100, loss = 0.0000\n",
      "training (95%) epoch 95/100, loss = 0.0001\n",
      "training (96%) epoch 96/100, loss = 0.0007\n",
      "training (97%) epoch 97/100, loss = 0.0008\n",
      "training (98%) epoch 98/100, loss = 0.0001\n",
      "training (99%) epoch 99/100, loss = 0.0001\n",
      "training (100%) epoch 100/100, loss = 0.3478\n"
     ]
    }
   ],
   "source": [
    "#ds.set_buffer_size(1024)\n",
    "nn.train_model(ds, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model with the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing 1% [4 / 6564 files]\n",
      "testing 2% [72 / 6564 files]\n",
      "testing 3% [140 / 6564 files]\n",
      "testing 4% [208 / 6564 files]\n",
      "testing 5% [276 / 6564 files]\n",
      "testing 6% [344 / 6564 files]\n",
      "testing 7% [412 / 6564 files]\n",
      "testing 8% [480 / 6564 files]\n",
      "testing 9% [548 / 6564 files]\n",
      "testing 10% [616 / 6564 files]\n",
      "testing 11% [684 / 6564 files]\n",
      "testing 12% [752 / 6564 files]\n",
      "testing 13% [820 / 6564 files]\n",
      "testing 14% [888 / 6564 files]\n",
      "testing 15% [956 / 6564 files]\n",
      "testing 16% [1024 / 6564 files]\n",
      "testing 17% [1092 / 6564 files]\n",
      "testing 18% [1160 / 6564 files]\n",
      "testing 19% [1228 / 6564 files]\n",
      "testing 20% [1296 / 6564 files]\n",
      "testing 21% [1364 / 6564 files]\n",
      "testing 22% [1432 / 6564 files]\n",
      "testing 23% [1500 / 6564 files]\n",
      "testing 24% [1568 / 6564 files]\n",
      "testing 25% [1636 / 6564 files]\n",
      "testing 26% [1704 / 6564 files]\n",
      "testing 27% [1772 / 6564 files]\n",
      "testing 28% [1840 / 6564 files]\n",
      "testing 29% [1908 / 6564 files]\n",
      "testing 30% [1976 / 6564 files]\n",
      "testing 31% [2044 / 6564 files]\n",
      "testing 32% [2112 / 6564 files]\n",
      "testing 33% [2180 / 6564 files]\n",
      "testing 34% [2248 / 6564 files]\n",
      "testing 35% [2316 / 6564 files]\n",
      "testing 36% [2384 / 6564 files]\n",
      "testing 37% [2452 / 6564 files]\n",
      "testing 38% [2520 / 6564 files]\n",
      "testing 39% [2588 / 6564 files]\n",
      "testing 40% [2656 / 6564 files]\n",
      "testing 41% [2724 / 6564 files]\n",
      "testing 42% [2792 / 6564 files]\n",
      "testing 43% [2860 / 6564 files]\n",
      "testing 44% [2928 / 6564 files]\n",
      "testing 45% [2996 / 6564 files]\n",
      "testing 46% [3064 / 6564 files]\n",
      "testing 47% [3132 / 6564 files]\n",
      "testing 48% [3200 / 6564 files]\n",
      "testing 49% [3268 / 6564 files]\n",
      "testing 50% [3336 / 6564 files]\n",
      "testing 51% [3404 / 6564 files]\n",
      "testing 52% [3472 / 6564 files]\n",
      "testing 53% [3540 / 6564 files]\n",
      "testing 54% [3608 / 6564 files]\n",
      "testing 55% [3676 / 6564 files]\n",
      "testing 56% [3744 / 6564 files]\n",
      "testing 57% [3812 / 6564 files]\n",
      "testing 58% [3880 / 6564 files]\n",
      "testing 59% [3948 / 6564 files]\n",
      "testing 60% [4016 / 6564 files]\n",
      "testing 61% [4084 / 6564 files]\n",
      "testing 62% [4152 / 6564 files]\n",
      "testing 63% [4220 / 6564 files]\n",
      "testing 64% [4288 / 6564 files]\n",
      "testing 65% [4356 / 6564 files]\n",
      "testing 66% [4424 / 6564 files]\n",
      "testing 67% [4492 / 6564 files]\n",
      "testing 68% [4560 / 6564 files]\n",
      "testing 69% [4628 / 6564 files]\n",
      "testing 70% [4696 / 6564 files]\n",
      "testing 71% [4764 / 6564 files]\n",
      "testing 72% [4832 / 6564 files]\n",
      "testing 73% [4900 / 6564 files]\n",
      "testing 74% [4968 / 6564 files]\n",
      "testing 75% [5036 / 6564 files]\n",
      "testing 76% [5104 / 6564 files]\n",
      "testing 77% [5172 / 6564 files]\n",
      "testing 78% [5240 / 6564 files]\n",
      "testing 79% [5308 / 6564 files]\n",
      "testing 80% [5376 / 6564 files]\n",
      "testing 81% [5444 / 6564 files]\n",
      "testing 82% [5512 / 6564 files]\n",
      "testing 83% [5580 / 6564 files]\n",
      "testing 84% [5648 / 6564 files]\n",
      "testing 85% [5716 / 6564 files]\n",
      "testing 86% [5784 / 6564 files]\n",
      "testing 87% [5852 / 6564 files]\n",
      "testing 88% [5920 / 6564 files]\n",
      "testing 89% [5988 / 6564 files]\n",
      "testing 90% [6056 / 6564 files]\n",
      "testing 91% [6124 / 6564 files]\n",
      "testing 92% [6192 / 6564 files]\n",
      "testing 93% [6260 / 6564 files]\n",
      "testing 94% [6328 / 6564 files]\n",
      "testing 95% [6396 / 6564 files]\n",
      "testing 96% [6464 / 6564 files]\n",
      "testing 97% [6532 / 6564 files]\n",
      "accuracy: 82.48% [43310/52512]\n",
      "\t - Normal: 92.34% [6061/6564]\n",
      "\t - Diabetes: 83.87% [5505/6564]\n",
      "\t - Glaucoma: 74.56% [4894/6564]\n",
      "\t - Cataract: 90.40% [5934/6564]\n",
      "\t - AgeRelatedMacularDegeneration: 83.68% [5493/6564]\n",
      "\t - Hypertension: 74.73% [4905/6564]\n",
      "\t - PathologicalMyopia: 92.00% [6039/6564]\n",
      "\t - Other: 68.24% [4479/6564]\n"
     ]
    }
   ],
   "source": [
    "nn.test_model(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the network with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn.save_layer_weights(0, \"eye_classification_net_full_resnet18.w\")\n",
    "nn.save_weights(\"eye_classification_net_full.w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 224\n",
    "\n",
    "apply_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=input_size),\n",
    "    transforms.CenterCrop(size=input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ds = EyeImageDataset(root=image_dir_testing, data_info_csv_file=csv_file, transform=apply_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing 1% [4 / 6564 files]\n",
      "testing 2% [72 / 6564 files]\n",
      "testing 3% [140 / 6564 files]\n",
      "testing 4% [208 / 6564 files]\n",
      "testing 5% [276 / 6564 files]\n",
      "testing 6% [344 / 6564 files]\n",
      "testing 7% [412 / 6564 files]\n",
      "testing 8% [480 / 6564 files]\n",
      "testing 9% [548 / 6564 files]\n",
      "testing 10% [616 / 6564 files]\n",
      "testing 11% [684 / 6564 files]\n",
      "testing 12% [752 / 6564 files]\n",
      "testing 13% [820 / 6564 files]\n",
      "testing 14% [888 / 6564 files]\n",
      "testing 15% [956 / 6564 files]\n",
      "testing 16% [1024 / 6564 files]\n",
      "testing 17% [1092 / 6564 files]\n",
      "testing 18% [1160 / 6564 files]\n",
      "testing 19% [1228 / 6564 files]\n",
      "testing 20% [1296 / 6564 files]\n",
      "testing 21% [1364 / 6564 files]\n",
      "testing 22% [1432 / 6564 files]\n",
      "testing 23% [1500 / 6564 files]\n",
      "testing 24% [1568 / 6564 files]\n",
      "testing 25% [1636 / 6564 files]\n",
      "testing 26% [1704 / 6564 files]\n",
      "testing 27% [1772 / 6564 files]\n",
      "testing 28% [1840 / 6564 files]\n",
      "testing 29% [1908 / 6564 files]\n",
      "testing 30% [1976 / 6564 files]\n",
      "testing 31% [2044 / 6564 files]\n",
      "testing 32% [2112 / 6564 files]\n",
      "testing 33% [2180 / 6564 files]\n",
      "testing 34% [2248 / 6564 files]\n",
      "testing 35% [2316 / 6564 files]\n",
      "testing 36% [2384 / 6564 files]\n",
      "testing 37% [2452 / 6564 files]\n",
      "testing 38% [2520 / 6564 files]\n",
      "testing 39% [2588 / 6564 files]\n",
      "testing 40% [2656 / 6564 files]\n",
      "testing 41% [2724 / 6564 files]\n",
      "testing 42% [2792 / 6564 files]\n",
      "testing 43% [2860 / 6564 files]\n",
      "testing 44% [2928 / 6564 files]\n",
      "testing 45% [2996 / 6564 files]\n",
      "testing 46% [3064 / 6564 files]\n",
      "testing 47% [3132 / 6564 files]\n",
      "testing 48% [3200 / 6564 files]\n",
      "testing 49% [3268 / 6564 files]\n",
      "testing 50% [3336 / 6564 files]\n",
      "testing 51% [3404 / 6564 files]\n",
      "testing 52% [3472 / 6564 files]\n",
      "testing 53% [3540 / 6564 files]\n",
      "testing 54% [3608 / 6564 files]\n",
      "testing 55% [3676 / 6564 files]\n",
      "testing 56% [3744 / 6564 files]\n",
      "testing 57% [3812 / 6564 files]\n",
      "testing 58% [3880 / 6564 files]\n",
      "testing 59% [3948 / 6564 files]\n",
      "testing 60% [4016 / 6564 files]\n",
      "testing 61% [4084 / 6564 files]\n",
      "testing 62% [4152 / 6564 files]\n",
      "testing 63% [4220 / 6564 files]\n",
      "testing 64% [4288 / 6564 files]\n",
      "testing 65% [4356 / 6564 files]\n",
      "testing 66% [4424 / 6564 files]\n",
      "testing 67% [4492 / 6564 files]\n",
      "testing 68% [4560 / 6564 files]\n",
      "testing 69% [4628 / 6564 files]\n",
      "testing 70% [4696 / 6564 files]\n",
      "testing 71% [4764 / 6564 files]\n",
      "testing 72% [4832 / 6564 files]\n",
      "testing 73% [4900 / 6564 files]\n",
      "testing 74% [4968 / 6564 files]\n",
      "testing 75% [5036 / 6564 files]\n",
      "testing 76% [5104 / 6564 files]\n",
      "testing 77% [5172 / 6564 files]\n",
      "testing 78% [5240 / 6564 files]\n",
      "testing 79% [5308 / 6564 files]\n",
      "testing 80% [5376 / 6564 files]\n",
      "testing 81% [5444 / 6564 files]\n",
      "testing 82% [5512 / 6564 files]\n",
      "testing 83% [5580 / 6564 files]\n",
      "testing 84% [5648 / 6564 files]\n",
      "testing 85% [5716 / 6564 files]\n",
      "testing 86% [5784 / 6564 files]\n",
      "testing 87% [5852 / 6564 files]\n",
      "testing 88% [5920 / 6564 files]\n",
      "testing 89% [5988 / 6564 files]\n",
      "testing 90% [6056 / 6564 files]\n",
      "testing 91% [6124 / 6564 files]\n",
      "testing 92% [6192 / 6564 files]\n",
      "testing 93% [6260 / 6564 files]\n",
      "testing 94% [6328 / 6564 files]\n",
      "testing 95% [6396 / 6564 files]\n",
      "testing 96% [6464 / 6564 files]\n",
      "testing 97% [6532 / 6564 files]\n",
      "accuracy: 82.48% [43310/52512]\n",
      "\t - Normal: 92.34% [6061/6564]\n",
      "\t - Diabetes: 83.87% [5505/6564]\n",
      "\t - Glaucoma: 74.56% [4894/6564]\n",
      "\t - Cataract: 90.40% [5934/6564]\n",
      "\t - AgeRelatedMacularDegeneration: 83.68% [5493/6564]\n",
      "\t - Hypertension: 74.73% [4905/6564]\n",
      "\t - PathologicalMyopia: 92.00% [6039/6564]\n",
      "\t - Other: 68.24% [4479/6564]\n"
     ]
    }
   ],
   "source": [
    "ds.set_buffer_size(16)\n",
    "nn.test_model(ds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
